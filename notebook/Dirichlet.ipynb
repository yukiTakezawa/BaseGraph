{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "%matplotlib inline\n",
    "from data.loader_dirichlet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def distribute_data_dirichlet(\n",
    "    targets, non_iid_alpha, n_workers, seed=0, num_auxiliary_workers=10\n",
    "):\n",
    "    # we refer https://github.com/epfml/relaysgd/tree/89719198ba227ebbff9a6bf5b61cb9baada167fd\n",
    "    \"\"\"Code adapted from Tao Lin (partition_data.py)\"\"\"\n",
    "    random_state = np.random.RandomState(seed=seed)\n",
    "\n",
    "    num_indices = len(targets)\n",
    "    num_classes = len(np.unique(targets))\n",
    "\n",
    "    indices2targets = np.array(list(enumerate(targets)))\n",
    "    random_state.shuffle(indices2targets)\n",
    "\n",
    "    # partition indices.\n",
    "    from_index = 0\n",
    "    splitted_targets = []\n",
    "    num_splits = math.ceil(n_workers / num_auxiliary_workers)\n",
    "    split_n_workers = [\n",
    "        num_auxiliary_workers\n",
    "        if idx < num_splits - 1\n",
    "        else n_workers - num_auxiliary_workers * (num_splits - 1)\n",
    "        for idx in range(num_splits)\n",
    "    ]\n",
    "    split_ratios = [_n_workers / n_workers for _n_workers in split_n_workers]\n",
    "    for idx, ratio in enumerate(split_ratios):\n",
    "        to_index = from_index + int(num_auxiliary_workers / n_workers * num_indices)\n",
    "        splitted_targets.append(\n",
    "            indices2targets[\n",
    "                from_index : (num_indices if idx == num_splits - 1 else to_index)\n",
    "            ]\n",
    "        )\n",
    "        from_index = to_index\n",
    "\n",
    "    idx_batch = []\n",
    "    for _targets in splitted_targets:\n",
    "        # rebuild _targets.\n",
    "        _targets = np.array(_targets)\n",
    "        _targets_size = len(_targets)\n",
    "\n",
    "        # use auxi_workers for this subset targets.\n",
    "        _n_workers = min(num_auxiliary_workers, n_workers)\n",
    "        n_workers = n_workers - num_auxiliary_workers\n",
    "\n",
    "        # get the corresponding idx_batch.\n",
    "        min_size = 0\n",
    "        while min_size < int(0.50 * _targets_size / _n_workers):\n",
    "            _idx_batch = [[] for _ in range(_n_workers)]\n",
    "            for _class in range(num_classes):\n",
    "                # get the corresponding indices in the original 'targets' list.\n",
    "                idx_class = np.where(_targets[:, 1] == _class)[0]\n",
    "                idx_class = _targets[idx_class, 0]\n",
    "\n",
    "                # sampling.\n",
    "                try:\n",
    "                    proportions = random_state.dirichlet(\n",
    "                        np.repeat(non_iid_alpha, _n_workers)\n",
    "                    )\n",
    "                    # balance\n",
    "                    proportions = np.array(\n",
    "                        [\n",
    "                            p * (len(idx_j) < _targets_size / _n_workers)\n",
    "                            for p, idx_j in zip(proportions, _idx_batch)\n",
    "                        ]\n",
    "                    )\n",
    "                    proportions = proportions / proportions.sum()\n",
    "                    proportions = (np.cumsum(proportions) * len(idx_class)).astype(int)[\n",
    "                        :-1\n",
    "                    ]\n",
    "                    _idx_batch = [\n",
    "                        idx_j + idx.tolist()\n",
    "                        for idx_j, idx in zip(\n",
    "                            _idx_batch, np.split(idx_class, proportions)\n",
    "                        )\n",
    "                    ]\n",
    "                    sizes = [len(idx_j) for idx_j in _idx_batch]\n",
    "                    min_size = min([_size for _size in sizes])\n",
    "                except ZeroDivisionError:\n",
    "                    pass\n",
    "        idx_batch += _idx_batch\n",
    "    return idx_batch\n",
    "\n",
    "\n",
    "## https://github.com/epfml/relaysgd/tree/89719198ba227ebbff9a6bf5b61cb9baada167fd\n",
    "def dirichlet_split(\n",
    "        dataset,\n",
    "        num_workers: int,\n",
    "        alpha: float = 1,\n",
    "        seed: int = 0,\n",
    "        distribute_evenly: bool = True,\n",
    "    ):\n",
    "        indices_per_worker = distribute_data_dirichlet(\n",
    "            dataset.targets, alpha, num_workers, num_auxiliary_workers=10, seed=seed\n",
    "        )\n",
    "\n",
    "        if distribute_evenly:\n",
    "            indices_per_worker = np.array_split(\n",
    "                np.concatenate(indices_per_worker), num_workers\n",
    "            )\n",
    "\n",
    "        return [\n",
    "            Subset(dataset, indices)\n",
    "            for indices in indices_per_worker\n",
    "        ]\n",
    "\n",
    "def datasets_to_loaders(datasets, batch_size=128):\n",
    "    \"\"\"\n",
    "    datasets dict:\n",
    "    \"\"\"\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets[\"train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, num_workers=2, pin_memory=False, persistent_workers=True)\n",
    "\n",
    "    all_train_loader = torch.utils.data.DataLoader(\n",
    "        datasets[\"all_train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets[\"val\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, num_workers=2, pin_memory=False)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets[\"test\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, num_workers=2, pin_memory=False)\n",
    "\n",
    "    return {\"train\": train_loader, \"val\": val_loader, \"all_train\": all_train_loader, \"test\": test_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_FMNIST(n_node, alpha=1.0, val_rate=0.2, seed=0):\n",
    "    \"\"\"\n",
    "    node_label : \n",
    "        the list of labes that each node has. (example. [[0,1],[1,2],[0,2]] (n_node=3, n_class=2))\n",
    "    \"\"\"\n",
    "\n",
    "    train_val_dataset = datasets.FashionMNIST('../data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.RandomCrop(28, padding=4),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST('../data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "    \n",
    "    print(f\"train_val: {len(train_val_dataset)}, test: {len(test_dataset)}\")\n",
    "    \n",
    "    # split datasets into n_node datasets by Dirichlet distribution. \n",
    "    train_val_subset_list = dirichlet_split(train_val_dataset, n_node, alpha, seed=seed)\n",
    "        \n",
    "    # the number of train datasets per node.\n",
    "    n_data = min([len(train_val_subset_list[node_id]) for node_id in range(n_node)])\n",
    "    n_train = int((1.0 - val_rate) * n_data)\n",
    "    \n",
    "    # choose validation datasets.\n",
    "    val_dataset = None\n",
    "    train_subset_list = []\n",
    "    for node_id in range(n_node):\n",
    "        n_val = len(train_val_subset_list[node_id]) - n_train\n",
    "        a, b = torch.utils.data.random_split(train_val_subset_list[node_id], [n_train, n_val])\n",
    "        train_subset_list.append(a)\n",
    "        \n",
    "        if val_dataset is None:\n",
    "            val_dataset = b\n",
    "        else:\n",
    "            val_dataset += b\n",
    "                  \n",
    "    return {'train': train_subset_list, 'val': val_dataset, 'test': test_dataset}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: 60000, test: 10000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 3\n",
    "my_datasets = load_FMNIST(n_nodes, alpha=0.1, val_rate=0.1, seed=0)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for key in my_datasets:\n",
    "    if key == \"train\":\n",
    "        for node_id in range(n_nodes):\n",
    "            count += len(my_datasets[key][node_id])\n",
    "    if key == \"val\":\n",
    "        count += len(my_datasets[key])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5161, 0, 5394, 0, 5375, 776, 8, 164, 1122, 0]\n",
      "[250, 5365, 7, 5413, 1, 4637, 887, 8, 864, 568]\n",
      "[0, 0, 0, 0, 0, 0, 4523, 5223, 3430, 4824]\n"
     ]
    }
   ],
   "source": [
    "for node_id in range(n_nodes):\n",
    "    counter = [0 for _ in range(10)]\n",
    "    \n",
    "    for data, label in my_datasets[\"train\"][node_id]:\n",
    "        counter[label] += 1\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[589, 635, 599, 587, 624, 587, 582, 605, 584, 608]\n"
     ]
    }
   ],
   "source": [
    "counter = [0 for _ in range(10)]\n",
    "    \n",
    "for data, label in my_datasets[\"val\"]:\n",
    "    counter[label] += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int( 0.9 * len(my_datasets[\"train\"][0]))\n",
    "n_val = len(my_datasets[\"train\"][0]) - int(0.9 * len(my_datasets[\"train\"][0])) \n",
    "\n",
    "\n",
    "a, b = torch.utils.data.random_split(my_datasets[\"train\"][0], [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "        a+b,\n",
    "        batch_size=1,\n",
    "        shuffle=True, num_workers=2, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3804, 3, 3393, 69, 445, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "counter = [0 for i in range(10)]\n",
    "for data, label in a:\n",
    "    counter[label] += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4245, 3, 3760, 74, 490, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "counter = [0 for i in range(10)]\n",
    "for data, label in loader:\n",
    "    counter[label.item()] += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4245, 3, 3760, 74, 490, 0, 0, 0, 0, 0]\n",
      "[543, 50, 383, 17, 423, 6, 172, 1433, 10, 5535]\n",
      "[0, 5, 1351, 1815, 0, 3840, 0, 1561, 0, 0]\n",
      "[807, 0, 0, 2155, 938, 2042, 212, 2417, 0, 0]\n",
      "[404, 3353, 0, 0, 3815, 0, 533, 1, 0, 465]\n",
      "[1, 2589, 506, 393, 0, 0, 5082, 0, 0, 0]\n",
      "[0, 0, 0, 1546, 334, 112, 1, 588, 5990, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(7):\n",
    "    label_count = [0 for _ in range(10)]\n",
    "\n",
    "    for data, label in my_datasets[\"train\"][i]:\n",
    "        label_count[label] += 1\n",
    "\n",
    "    print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = datasets.FashionMNIST('../data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.RandomCrop(28, padding=4),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_datasets = load_FMNIST(7, batch=100, alpha=0.1, val_rate=0.1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4245, 3, 3760, 74, 490, 0, 0, 0, 0, 0]\n",
      "[543, 50, 383, 17, 423, 6, 172, 1433, 10, 5535]\n",
      "[0, 5, 1351, 1815, 0, 3840, 0, 1561, 0, 0]\n",
      "[807, 0, 0, 2155, 938, 2042, 212, 2417, 0, 0]\n",
      "[404, 3353, 0, 0, 3815, 0, 533, 1, 0, 465]\n",
      "[1, 2589, 506, 393, 0, 0, 5082, 0, 0, 0]\n",
      "[0, 0, 0, 1546, 334, 112, 1, 588, 5990, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(7):\n",
    "    label_count = [0 for _ in range(10)]\n",
    "\n",
    "    for data, label in my_datasets[\"train\"][i]:\n",
    "        label_count[label] += 1\n",
    "\n",
    "    print(label_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
