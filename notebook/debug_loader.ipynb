{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5800c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import random\n",
    "from torch.utils.data.dataset import Subset\n",
    "from more_itertools import chunked\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def distribute_data_dirichlet(\n",
    "    targets, non_iid_alpha, n_workers, seed=0, num_auxiliary_workers=10\n",
    "):\n",
    "    # we refer https://github.com/epfml/relaysgd/tree/89719198ba227ebbff9a6bf5b61cb9baada167fd\n",
    "    \"\"\"Code adapted from Tao Lin (partition_data.py)\"\"\"\n",
    "    random_state = np.random.RandomState(seed=seed)\n",
    "\n",
    "    num_indices = len(targets)\n",
    "    num_classes = len(np.unique(targets))\n",
    "\n",
    "    indices2targets = np.array(list(enumerate(targets)))\n",
    "    random_state.shuffle(indices2targets)\n",
    "\n",
    "    # partition indices.\n",
    "    from_index = 0\n",
    "    splitted_targets = []\n",
    "    num_splits = math.ceil(n_workers / num_auxiliary_workers)\n",
    "    split_n_workers = [\n",
    "        num_auxiliary_workers\n",
    "        if idx < num_splits - 1\n",
    "        else n_workers - num_auxiliary_workers * (num_splits - 1)\n",
    "        for idx in range(num_splits)\n",
    "    ]\n",
    "    split_ratios = [_n_workers / n_workers for _n_workers in split_n_workers]\n",
    "    for idx, ratio in enumerate(split_ratios):\n",
    "        to_index = from_index + int(num_auxiliary_workers / n_workers * num_indices)\n",
    "        splitted_targets.append(\n",
    "            indices2targets[\n",
    "                from_index : (num_indices if idx == num_splits - 1 else to_index)\n",
    "            ]\n",
    "        )\n",
    "        from_index = to_index\n",
    "\n",
    "    idx_batch = []\n",
    "    for _targets in splitted_targets:\n",
    "        # rebuild _targets.\n",
    "        _targets = np.array(_targets)\n",
    "        _targets_size = len(_targets)\n",
    "\n",
    "        # use auxi_workers for this subset targets.\n",
    "        _n_workers = min(num_auxiliary_workers, n_workers)\n",
    "        n_workers = n_workers - num_auxiliary_workers\n",
    "\n",
    "        # get the corresponding idx_batch.\n",
    "        min_size = 0\n",
    "        while min_size < int(0.50 * _targets_size / _n_workers):\n",
    "            _idx_batch = [[] for _ in range(_n_workers)]\n",
    "            for _class in range(num_classes):\n",
    "                # get the corresponding indices in the original 'targets' list.\n",
    "                idx_class = np.where(_targets[:, 1] == _class)[0]\n",
    "                idx_class = _targets[idx_class, 0]\n",
    "\n",
    "                # sampling.\n",
    "                try:\n",
    "                    proportions = random_state.dirichlet(\n",
    "                        np.repeat(non_iid_alpha, _n_workers)\n",
    "                    )\n",
    "                    # balance\n",
    "                    proportions = np.array(\n",
    "                        [\n",
    "                            p * (len(idx_j) < _targets_size / _n_workers)\n",
    "                            for p, idx_j in zip(proportions, _idx_batch)\n",
    "                        ]\n",
    "                    )\n",
    "                    proportions = proportions / proportions.sum()\n",
    "                    proportions = (np.cumsum(proportions) * len(idx_class)).astype(int)[\n",
    "                        :-1\n",
    "                    ]\n",
    "                    _idx_batch = [\n",
    "                        idx_j + idx.tolist()\n",
    "                        for idx_j, idx in zip(\n",
    "                            _idx_batch, np.split(idx_class, proportions)\n",
    "                        )\n",
    "                    ]\n",
    "                    sizes = [len(idx_j) for idx_j in _idx_batch]\n",
    "                    min_size = min([_size for _size in sizes])\n",
    "                except ZeroDivisionError:\n",
    "                    pass\n",
    "        idx_batch += _idx_batch\n",
    "    return idx_batch\n",
    "\n",
    "\n",
    "## https://github.com/epfml/relaysgd/tree/89719198ba227ebbff9a6bf5b61cb9baada167fd\n",
    "def dirichlet_split(\n",
    "        dataset,\n",
    "        num_workers: int,\n",
    "        alpha: float = 1,\n",
    "        seed: int = 0,\n",
    "        distribute_evenly: bool = True,\n",
    "    ):\n",
    "        indices_per_worker = distribute_data_dirichlet(\n",
    "            dataset.targets, alpha, num_workers, num_auxiliary_workers=10, seed=seed\n",
    "        )\n",
    "\n",
    "        if distribute_evenly:\n",
    "            indices_per_worker = np.array_split(\n",
    "                np.concatenate(indices_per_worker), num_workers\n",
    "            )\n",
    "\n",
    "        return [\n",
    "            Subset(dataset, indices)\n",
    "            for indices in indices_per_worker\n",
    "        ]\n",
    "\n",
    "    \n",
    "def load_CIFAR10(n_node, alpha=1.0, batch=128, val_rate=0.2, seed=0):\n",
    "    \"\"\"\n",
    "    node_label : \n",
    "        the list of labes that each node has. (example. [[0,1],[1,2],[0,2]] (n_node=3, n_class=2))\n",
    "    \"\"\"\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.RandomErasing(),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    train_val_dataset = datasets.CIFAR10('../data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transform_train)\n",
    "    \n",
    "    test_dataset = datasets.CIFAR10('../data',\n",
    "                       train=False,\n",
    "                       transform=transform_test)\n",
    "\n",
    "\n",
    "    # split datasets into n_node datasets by Dirichlet distribution. \n",
    "    train_val_subset_list = dirichlet_split(train_val_dataset, n_node, alpha, seed=seed)\n",
    "        \n",
    "    # the number of train datasets per node.\n",
    "    n_data = min([len(train_val_subset_list[node_id]) for node_id in range(n_node)])\n",
    "    n_train = int((1.0 - val_rate) * n_data)\n",
    "    \n",
    "    # choose validation datasets.\n",
    "    val_dataset = None\n",
    "    train_subset_list = []\n",
    "    for node_id in range(n_node):\n",
    "        n_val = len(train_val_subset_list[node_id]) - n_train\n",
    "        a, b = torch.utils.data.random_split(train_val_subset_list[node_id], [n_train, n_val])\n",
    "        train_subset_list.append(a)\n",
    "        \n",
    "        if val_dataset is None:\n",
    "            val_dataset = b\n",
    "        else:\n",
    "            val_dataset += b\n",
    "                  \n",
    "    return {'train': train_subset_list, 'val': val_dataset, 'test': test_dataset}\n",
    "\n",
    "\n",
    "def load_CIFAR100(n_node, alpha=1.0, batch=128, val_rate=0.2, seed=0):\n",
    "    \"\"\"\n",
    "    node_label : \n",
    "        the list of labes that each node has. (example. [[0,1],[1,2],[0,2]] (n_node=3, n_class=2))\n",
    "    \"\"\"\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.RandomErasing(),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    train_val_dataset = datasets.CIFAR100('../data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transform_train)\n",
    "    \n",
    "    test_dataset = datasets.CIFAR100('../data',\n",
    "                       train=False,\n",
    "                       transform=transform_test)\n",
    "\n",
    "    # split datasets into n_node datasets by Dirichlet distribution. \n",
    "    train_val_subset_list = dirichlet_split(train_val_dataset, n_node, alpha, seed=seed)\n",
    "        \n",
    "    # the number of train datasets per node.\n",
    "    n_data = min([len(train_val_subset_list[node_id]) for node_id in range(n_node)])\n",
    "    n_train = int((1.0 - val_rate) * n_data)\n",
    "    \n",
    "    # choose validation datasets.\n",
    "    val_dataset = None\n",
    "    train_subset_list = []\n",
    "    for node_id in range(n_node):\n",
    "        n_val = len(train_val_subset_list[node_id]) - n_train\n",
    "        a, b = torch.utils.data.random_split(train_val_subset_list[node_id], [n_train, n_val])\n",
    "        train_subset_list.append(a)\n",
    "        \n",
    "        if val_dataset is None:\n",
    "            val_dataset = b\n",
    "        else:\n",
    "            val_dataset += b\n",
    "                  \n",
    "    return {'train': train_subset_list, 'val': val_dataset, 'test': test_dataset}\n",
    "\n",
    "\n",
    "def load_FMNIST(n_node, alpha=1.0, val_rate=0.2, seed=0):\n",
    "    \"\"\"\n",
    "    node_label : \n",
    "        the list of labes that each node has. (example. [[0,1],[1,2],[0,2]] (n_node=3, n_class=2))\n",
    "    \"\"\"\n",
    "\n",
    "    train_val_dataset = datasets.FashionMNIST('../data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.RandomCrop(28, padding=4),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST('../data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "    \n",
    "    # split datasets into n_node datasets by Dirichlet distribution. \n",
    "    train_val_subset_list = dirichlet_split(train_val_dataset, n_node, alpha, seed=seed)\n",
    "        \n",
    "    # the number of train datasets per node.\n",
    "    n_data = min([len(train_val_subset_list[node_id]) for node_id in range(n_node)])\n",
    "    n_train = int((1.0 - val_rate) * n_data)\n",
    "    \n",
    "    # choose validation datasets.\n",
    "    val_dataset = None\n",
    "    train_subset_list = []\n",
    "    for node_id in range(n_node):\n",
    "        n_val = len(train_val_subset_list[node_id]) - n_train\n",
    "        a, b = torch.utils.data.random_split(train_val_subset_list[node_id], [n_train, n_val])\n",
    "        train_subset_list.append(a)\n",
    "        \n",
    "        if val_dataset is None:\n",
    "            val_dataset = b\n",
    "        else:\n",
    "            val_dataset += b\n",
    "                  \n",
    "    return {'train': train_subset_list, 'val': val_dataset, 'test': test_dataset}\n",
    "                                \n",
    "\n",
    "def datasets_to_loaders(datasets, batch_size=128):\n",
    "    \"\"\"\n",
    "    datasets dict:\n",
    "    \"\"\"\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets[\"train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "    all_train_loader = torch.utils.data.DataLoader(\n",
    "        datasets[\"all_train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets[\"val\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, num_workers=2, pin_memory=False)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets[\"test\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, num_workers=2, pin_memory=False)\n",
    "\n",
    "    return {\"train\": train_loader, \"val\": val_loader, \"all_train\": all_train_loader, \"test\": test_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e6efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "[180, 643, 14, 0, 963, 0, 0, 0, 0, 0]\n",
      "[56, 131, 78, 0, 744, 791, 0, 0, 0, 0]\n",
      "[355, 559, 103, 0, 0, 783, 0, 0, 0, 0]\n",
      "[0, 1, 44, 1, 2, 4, 1098, 200, 450, 0]\n",
      "[0, 0, 21, 558, 0, 0, 0, 0, 1221, 0]\n",
      "[0, 171, 14, 603, 14, 0, 668, 327, 2, 1]\n",
      "[0, 18, 936, 0, 0, 0, 0, 846, 0, 0]\n",
      "[1, 3, 552, 417, 0, 225, 0, 80, 160, 362]\n",
      "[169, 0, 35, 48, 82, 0, 1, 1, 0, 1464]\n",
      "[1016, 284, 32, 165, 1, 4, 1, 296, 1, 0]\n",
      "[1174, 265, 224, 4, 0, 0, 0, 50, 0, 83]\n",
      "[151, 19, 64, 586, 101, 0, 0, 407, 472, 0]\n",
      "[0, 0, 0, 290, 0, 290, 1220, 0, 0, 0]\n",
      "[42, 39, 2, 917, 3, 0, 33, 0, 763, 1]\n",
      "[0, 318, 0, 0, 39, 2, 501, 940, 0, 0]\n",
      "[50, 648, 66, 6, 15, 0, 0, 312, 0, 703]\n",
      "[0, 296, 0, 35, 1469, 0, 0, 0, 0, 0]\n",
      "[0, 0, 23, 1, 178, 1434, 3, 0, 0, 161]\n",
      "[247, 0, 0, 0, 0, 49, 1, 136, 487, 880]\n",
      "[150, 224, 1426, 0, 0, 0, 0, 0, 0, 0]\n",
      "[141, 5, 26, 0, 0, 1503, 0, 122, 3, 0]\n",
      "[144, 13, 0, 0, 0, 0, 0, 13, 1630, 0]\n",
      "[105, 350, 2, 168, 0, 0, 0, 849, 149, 177]\n",
      "[285, 0, 0, 1515, 0, 0, 0, 0, 0, 0]\n",
      "[653, 806, 1, 0, 0, 166, 3, 157, 14, 0]\n",
      "[0, 405, 1184, 1, 210, 0, 0, 0, 0, 0]\n",
      "[0, 0, 463, 0, 631, 1, 40, 665, 0, 0]\n",
      "[0, 0, 0, 0, 953, 42, 805, 0, 0, 0]\n",
      "[0, 218, 98, 0, 1, 0, 1005, 0, 31, 447]\n",
      "[473, 1, 1, 94, 1, 92, 1, 1, 1, 1135]\n",
      "validation\n",
      "[608, 583, 591, 591, 593, 614, 620, 598, 616, 586]\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 30\n",
    "my_datasets = load_FMNIST(n_nodes, alpha=0.1, val_rate=0.1, seed=0)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for key in my_datasets:\n",
    "    if key == \"train\":\n",
    "        for node_id in range(n_nodes):\n",
    "            count += len(my_datasets[key][node_id])\n",
    "    if key == \"val\":\n",
    "        count += len(my_datasets[key])\n",
    "print(count)\n",
    "\n",
    "for node_id in range(n_nodes):\n",
    "    counter = [0 for _ in range(10)]\n",
    "    \n",
    "    for data, label in my_datasets[\"train\"][node_id]:\n",
    "        counter[label] += 1\n",
    "    print(counter)\n",
    "    \n",
    "print(\"validation\")\n",
    "counter = [0 for _ in range(10)]\n",
    "    \n",
    "for data, label in my_datasets[\"val\"]:\n",
    "    counter[label] += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11ef210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n",
      "[4477, 2507, 2807, 2, 486, 68, 136, 0, 0, 4516]\n",
      "[1, 1516, 1068, 2344, 1, 4439, 1169, 4461, 0, 0]\n",
      "[0, 475, 606, 2138, 4011, 1, 3183, 73, 4512, 0]\n",
      "validation\n",
      "[522, 502, 519, 516, 502, 492, 512, 466, 488, 484]\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 3\n",
    "my_datasets = load_CIFAR10(n_nodes, alpha=0.1, val_rate=0.1, seed=0)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for key in my_datasets:\n",
    "    if key == \"train\":\n",
    "        for node_id in range(n_nodes):\n",
    "            count += len(my_datasets[key][node_id])\n",
    "    if key == \"val\":\n",
    "        count += len(my_datasets[key])\n",
    "print(count)\n",
    "\n",
    "for node_id in range(n_nodes):\n",
    "    counter = [0 for _ in range(10)]\n",
    "    \n",
    "    for data, label in my_datasets[\"train\"][node_id]:\n",
    "        counter[label] += 1\n",
    "    print(counter)\n",
    "    \n",
    "print(\"validation\")\n",
    "counter = [0 for _ in range(10)]\n",
    "    \n",
    "for data, label in my_datasets[\"val\"]:\n",
    "    counter[label] += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1901e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n",
      "[445, 0, 274, 0, 49, 6, 12, 0, 0, 448, 447, 395, 288, 76, 0, 259, 24, 0, 122, 38, 0, 0, 1, 450, 1, 0, 0, 425, 0, 0, 428, 456, 0, 293, 37, 46, 13, 0, 384, 15, 0, 0, 5, 438, 173, 0, 6, 196, 455, 0, 0, 196, 445, 448, 0, 437, 0, 0, 416, 3, 0, 0, 451, 342, 0, 353, 220, 0, 0, 0, 187, 0, 0, 360, 92, 0, 429, 0, 369, 208, 397, 0, 5, 83, 206, 3, 37, 0, 402, 20, 0, 0, 328, 0, 283, 378, 452, 456, 0, 288]\n",
      "[1, 290, 111, 229, 0, 456, 122, 443, 461, 0, 5, 1, 148, 138, 105, 107, 0, 13, 171, 0, 0, 428, 445, 6, 451, 1, 0, 25, 0, 306, 15, 0, 454, 0, 412, 410, 432, 0, 61, 424, 434, 456, 0, 0, 283, 457, 202, 0, 0, 392, 448, 0, 0, 0, 0, 0, 0, 448, 0, 0, 220, 0, 0, 96, 0, 68, 0, 0, 110, 400, 266, 8, 0, 93, 358, 47, 0, 411, 63, 2, 50, 262, 348, 13, 230, 1, 416, 2, 53, 93, 176, 437, 110, 0, 176, 80, 1, 1, 447, 170]\n",
      "[0, 155, 65, 215, 393, 1, 317, 6, 1, 1, 0, 48, 1, 236, 343, 71, 424, 430, 157, 413, 467, 21, 2, 1, 0, 450, 443, 1, 442, 142, 1, 1, 1, 157, 1, 0, 1, 450, 1, 1, 2, 1, 452, 1, 1, 1, 243, 252, 4, 59, 1, 266, 1, 1, 449, 4, 467, 2, 41, 453, 233, 435, 0, 1, 452, 40, 230, 451, 341, 47, 1, 455, 444, 1, 0, 406, 11, 35, 26, 251, 0, 194, 104, 353, 1, 443, 1, 435, 3, 327, 271, 3, 1, 441, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 3\n",
    "my_datasets = load_CIFAR100(n_nodes, alpha=0.1, val_rate=0.1, seed=0)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for key in my_datasets:\n",
    "    if key == \"train\":\n",
    "        for node_id in range(n_nodes):\n",
    "            count += len(my_datasets[key][node_id])\n",
    "    if key == \"val\":\n",
    "        count += len(my_datasets[key])\n",
    "print(count)\n",
    "\n",
    "for node_id in range(n_nodes):\n",
    "    counter = [0 for _ in range(100)]\n",
    "    \n",
    "    for data, label in my_datasets[\"train\"][node_id]:\n",
    "        counter[label] += 1\n",
    "    print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "base2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
